{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ускорение обучения, начальные веса, стандартизация, подготовка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рекомендация 1\n",
    "- запускать алгоритм для разных начальных значений весовых коэффициентов. И, затем, отобрать лучший вариант. Начальные значения генерируем случайным образом в окресности нуля, кроме тех, что относятся к bias`сам.\n",
    "\n",
    "#### Рекомендация 2\n",
    "- запускаем алгоритм обучения с оптимизацией по Adam или Нестерову для ускорения обучения НС.\n",
    "\n",
    "#### Рекомендация 3\n",
    "- выполнять нормировку входных значений и запоминать нормировочные параметры min, max из обучающей выборки.\n",
    "\n",
    "#### Рекомендация 4\n",
    "- помещать в обучающую выборку самые разнообразные данные примерно равного количества.\n",
    "\n",
    "#### Рекомендация 5\n",
    "- наблюдения на вход сети подавать случайным образом, корректировать веса после серии наблюдейний, разбитых по mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Переобучение - что это и как этого избежать, критерии останова обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рекомендация 6\n",
    "- использовать минимальное необходимое число нейронов в нейронной сети.\n",
    "\n",
    "#### Рекомендация 7\n",
    "- разбивать все множество наблюдений на три выборки: обучающую, валидации и тестовую.\n",
    "\n",
    "#### Критерии останова процесса обучения\n",
    "- расхождение показателя качества для обучающей выборки и валидации;\n",
    "- от итерации к итерации (по всей эпохе) показатель качества Q практически не меняется (возможно надо изменить веса или переинициализировать НС);\n",
    "- происходит малое изменение весовых коэффициентов (возможно достигли минимального значения);\n",
    "- достигли максимальное число итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Функции активации, критерии качества работы НС"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рекомендаци 8\n",
    "- при малом числе слоев можно использовать гиперболическую и сигмоидальную функции активации или ReLu, при числе слоев от 8 и более - ReLu и ее вариации.\n",
    "\n",
    "#### Рекомендаци 9\n",
    "- для задач регрессии у выходных нейронов использовать линейную (linear) функцию активации, для задач классификации не пересекающихся классов - softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Критерий качества НС\n",
    "\n",
    "#### Распознавание: \n",
    "- хиндж (hinge);\n",
    "- бинарная кросс-энтропия (binary crossentropy) - при классификации двух классов;\n",
    "- категориальная кросс-энтропия (categorical crossentropy) - при классификации более двух классов;\n",
    "\n",
    "### Обработка текста:\n",
    "- логарифмический гиперболический косинус (logcos);\n",
    "\n",
    "### Задачи регрессии:\n",
    "- средний квадрат ошибок (mean squared error);\n",
    "- средний модуль ошибок (mean absolute error);\n",
    "- средний абсолютный процент ошибок (mean absolute percentage error) - хороша в прогнозировании;\n",
    "- средний квадрат логарифмических ошибок (mean squared logaritmic error)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
