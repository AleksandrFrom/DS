{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Делаем модель нейросети для распознавания рукописных цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 22:45:59.730527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 22:45:59.741858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 22:45:59.745053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.asarray(x_train).astype(np.float32)\n",
    "#y_train = np.asarray(y_train).astype(np.float32)\n",
    "#x_test = np.asarray(x_test).astype(np.float32)\n",
    "#y_test = np.asarray(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.reshape(tf.cast(x_train, tf.float32), [-1, 28*28])\n",
    "x_test = tf.reshape(tf.cast(x_test, tf.float32), [-1, 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(tf.Module):\n",
    "    def __init__(self, outputs, activate='relu'):\n",
    "        super().__init__()\n",
    "        self.outputs = outputs\n",
    "        self.activate = activate\n",
    "        self.fl_init = False\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.fl_init:\n",
    "            self.w = tf.random.truncated_normal((x.shape[-1], self.outputs), stddev=0.1, name='w')  \n",
    "            self.b = tf.zeros([self.outputs], dtype=tf.float32, name='b')\n",
    "            \n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b)\n",
    "            \n",
    "            self.fl_init = True\n",
    "            \n",
    "        y = x @ self.w + self.b\n",
    "        \n",
    "        if self.activate == 'relu':\n",
    "            return tf.nn.relu(y)\n",
    "        elif self.activate == 'softmax':\n",
    "            return tf.nn.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = DenseNN(128)\n",
    "layer_2 = DenseNN(10, activate='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(x):\n",
    "    y = layer_1(x)\n",
    "    y = layer_2(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = lambda y_true, y_pred: tf.reduce_mean(tf.losses.categorical_crossentropy(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "total = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402.81558\n",
      "219.29317\n",
      "162.4839\n",
      "127.879745\n",
      "103.4398\n",
      "89.784744\n",
      "70.23408\n",
      "64.2272\n",
      "51.867447\n",
      "41.186726\n"
     ]
    }
   ],
   "source": [
    "for n in range(epochs):\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            f_loss = cross_entropy(y_batch, model_predict(x_batch))\n",
    "            \n",
    "        loss += f_loss\n",
    "        grads = tape.gradient(f_loss, [layer_1.trainable_variables, layer_2.trainable_variables])\n",
    "        opt.apply_gradients(zip(grads[0], layer_1.trainable_variables))\n",
    "        #opt.apply_gradients(zip(grads[1], layer_2.trainable_variables))\n",
    "        \n",
    "    print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.77\n"
     ]
    }
   ],
   "source": [
    "y = model_predict(x_test)\n",
    "y2 = tf.argmax(y, axis=1).numpy()\n",
    "acc = len(y_test[y_test == y2]) / y_test.shape[0] * 100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(tf.Module):\n",
    "    def __init__(self, outputs, activate='relu'):\n",
    "        super().__init__()\n",
    "        self.outputs = outputs\n",
    "        self.activate = activate\n",
    "        self.fl_init = False\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.fl_init:\n",
    "            self.w = tf.random.truncated_normal((x.shape[-1], self.outputs), stddev=0.1, name='w')  \n",
    "            self.b = tf.zeros([self.outputs], dtype=tf.float32, name='b')\n",
    "            \n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b, trainable=False) #необучаемый параметр. всегда равен 0\n",
    "            \n",
    "            self.fl_init = True\n",
    "            \n",
    "        y = x @ self.w + self.b\n",
    "        \n",
    "        if self.activate == 'relu':\n",
    "            return tf.nn.relu(y)\n",
    "        elif self.activate == 'softmax':\n",
    "            return tf.nn.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 5\u001b[0m         f_loss \u001b[38;5;241m=\u001b[39m cross_entropy(y_batch, \u001b[43mmodel_predict\u001b[49m(x_batch))\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m f_loss\n\u001b[1;32m      8\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(f_loss, [layer_1\u001b[38;5;241m.\u001b[39mtrainable_variables, layer_2\u001b[38;5;241m.\u001b[39mtrainable_variables])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_predict' is not defined"
     ]
    }
   ],
   "source": [
    "for n in range(epochs):\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            f_loss = cross_entropy(y_batch, model_predict(x_batch))\n",
    "            \n",
    "        loss += f_loss\n",
    "        grads = tape.gradient(f_loss, [layer_1.trainable_variables, layer_2.trainable_variables])\n",
    "        opt.apply_gradients(zip(grads[0], layer_1.trainable_variables))\n",
    "        #opt.apply_gradients(zip(grads[1], layer_2.trainable_variables))\n",
    "        \n",
    "    print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.52\n"
     ]
    }
   ],
   "source": [
    "y = model_predict(x_test)\n",
    "y2 = tf.argmax(y, axis=1).numpy()\n",
    "acc = len(y_test[y_test == y2]) / y_test.shape[0] * 100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "параметр b мало влияет на результат, поэтому асс практически не изменилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### улучшенная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(tf.Module):\n",
    "    def __init__(self, outputs, activate='relu'):\n",
    "        super().__init__()\n",
    "        self.outputs = outputs\n",
    "        self.activate = activate\n",
    "        self.fl_init = False\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.fl_init:\n",
    "            self.w = tf.random.truncated_normal((x.shape[-1], self.outputs), stddev=0.1, name='w')  \n",
    "            self.b = tf.zeros([self.outputs], dtype=tf.float32, name='b')\n",
    "            \n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b)             \n",
    "            self.fl_init = True\n",
    "            \n",
    "        y = x @ self.w + self.b\n",
    "        \n",
    "        if self.activate == 'relu':\n",
    "            return tf.nn.relu(y)\n",
    "        elif self.activate == 'softmax':\n",
    "            return tf.nn.softmax(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class SequentialModule(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = DenseNN(128)\n",
    "        self.layer_2 = DenseNN(10, activate='softmax')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.layer_2(self.layer_1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModule()\n",
    "\n",
    "cross_entropy = lambda y_true, y_pred: tf.reduce_mean(tf.losses.categorical_crossentropy(y_true, y_pred))\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "total = x_train.shape[0]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440.3526\n",
      "299.5529\n",
      "265.71024\n",
      "244.24823\n",
      "229.31699\n",
      "219.79309\n",
      "223.50606\n",
      "196.91269\n",
      "193.9259\n",
      "189.78833\n"
     ]
    }
   ],
   "source": [
    "for n in range(epochs):\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            f_loss = cross_entropy(y_batch, model(x_batch))\n",
    "            \n",
    "        loss += f_loss\n",
    "        grads = tape.gradient(f_loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.38\n"
     ]
    }
   ],
   "source": [
    "y = model(x_test)\n",
    "y2 = tf.argmax(y, axis=1).numpy()\n",
    "acc = len(y_test[y_test == y2]) / y_test.shape[0] * 100\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
